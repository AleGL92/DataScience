{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP PREDICTIONS FOR YELP AND AMAZON REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text  sentiment\n",
      "0           0  So there is no way for me to plug it in here i...          0\n",
      "1           1                         Good case Excellent value.          1\n",
      "2           2                             Great for the jawbone.          1\n",
      "3           3  Tied to charger for conversations lasting more...          0\n",
      "4           4                                  The mic is great.          1\n"
     ]
    }
   ],
   "source": [
    "# The objective here is to prepare the data for a NLP prediction, using this Yelp and Amazon reviews DS.\n",
    "# The Dataset contains a review and a label, 0 or 1, expressing the sentiment.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "path = 'https://raw.githubusercontent.com/AleGL92/TensorFlow/main/NLP/combined_data.csv'\n",
    "ds = pd.read_csv(path)\n",
    "print(ds.head())\n",
    "reviews = list(ds['text'])\n",
    "# print(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3261 different words\n",
      "(1992, 139)\n",
      "So there is no way for me to plug it in here in the US unless I go by a converter.\n",
      "[  28   59    8   56  142   13   61    7  269    6   15   46   15    2\n",
      "  149  449    4   60  113    5 1429    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(oov_token = '<OOV>')\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "word_index = tokenizer.word_index\n",
    "print(f'There are {len(word_index)} different words')\n",
    "# print(word_index)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "padded_sequences = pad_sequences(sequences, padding='post')     # Post makes the zeros padded appear after the sequence\n",
    "\n",
    "print(padded_sequences.shape)       # The shape shows the number of sequences and the length of each one.\n",
    "print(reviews[0])               # Printing the first review in words\n",
    "print(padded_sequences[0])      # Printing the first review in sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING A BASIC SENTIMENT MODEL WITH EMBEDDINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text  sentiment\n",
      "0           0  So there is no way for me to plug it in here i...          0\n",
      "1           1                         Good case Excellent value.          1\n",
      "2           2                             Great for the jawbone.          1\n",
      "3           3  Tied to charger for conversations lasting more...          0\n",
      "4           4                                  The mic is great.          1\n"
     ]
    }
   ],
   "source": [
    "# We'll be using the same DS as before, but we will use labels now.\n",
    "\n",
    "path = 'https://raw.githubusercontent.com/AleGL92/TensorFlow/main/NLP/combined_data.csv'\n",
    "ds = pd.read_csv(path)\n",
    "print(ds.head())\n",
    "reviews = list(ds['text'])\n",
    "labels = list(ds['sentiment'])\n",
    "\n",
    "# We dont have something like train_test_split here, so we'll separate the traning and evaluation data manually\n",
    "training_size = int(len(reviews) * 0.8)\n",
    "training_sentences = reviews[0:training_size]\n",
    "testing_sentences = reviews[training_size:]\n",
    "training_labels = labels[0:training_size]\n",
    "testing_labels = labels[training_size:]\n",
    "# Make labels into numpy arrays for use with the network later\n",
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000           # Maximun number of words\n",
    "embedding_dim = 16          # Maximun number of possible sentiments\n",
    "max_length = 100            # Maximun lenght of the sequences\n",
    "trunc_type='post'           # Truncate the end of the sequences\n",
    "padding_type='post'         # Pad the end of the sequences\n",
    "oov_tok = \"<OOV>\"           # Out Of Vocabulary token\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences, maxlen = max_length, padding = padding_type, truncating = trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen = max_length, padding = padding_type, truncating = trunc_type)\n",
    "# print(word_index)         # {'<OOV>': 1, 'the': 2, 'and': 3, 'i': 4, 'it': 5, 'a': 6, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 20  90  76 364   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n",
      "good case excellent value ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "Good case Excellent value.\n"
     ]
    }
   ],
   "source": [
    "# Checking the preparation results\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])    # Just reversing key-value for value-key\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])         # The symbol ? represents 0s. When we use get, if the value is\n",
    "    # not provided, we will get the '?' string\n",
    "    # .join sets a space everytime we add an element to the list. get returns the value for a given key in a dictionary.\n",
    "\n",
    "print(padded[1])\n",
    "print(decode_review(padded[1]))\n",
    "print(training_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 100, 16)           16000     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 6)                 9606      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,613\n",
      "Trainable params: 25,613\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "50/50 [==============================] - 1s 4ms/step - loss: 0.6919 - accuracy: 0.5204 - val_loss: 0.7001 - val_accuracy: 0.4110\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6868 - accuracy: 0.5367 - val_loss: 0.7060 - val_accuracy: 0.4110\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6677 - accuracy: 0.5725 - val_loss: 0.6916 - val_accuracy: 0.4336\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6265 - accuracy: 0.6428 - val_loss: 0.6793 - val_accuracy: 0.4687\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5624 - accuracy: 0.7320 - val_loss: 0.6542 - val_accuracy: 0.5614\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4668 - accuracy: 0.8462 - val_loss: 0.5239 - val_accuracy: 0.7494\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2799 - accuracy: 0.9422 - val_loss: 0.4681 - val_accuracy: 0.7694\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1877 - accuracy: 0.9554 - val_loss: 0.4799 - val_accuracy: 0.7494\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.1351 - accuracy: 0.9692 - val_loss: 0.5921 - val_accuracy: 0.7068\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.0997 - accuracy: 0.9837 - val_loss: 0.5166 - val_accuracy: 0.7469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e85dd0c9a0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we train the sentiment model with embeddings. The embedding layer is first, and the output is only 1 node as it is either \n",
    "# 0 or 1 (negative or positive)\n",
    "import tensorflow as tf\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])     \n",
    "# This are the parameters recommended by TF for NLP and tokenization models \n",
    "model.summary()\n",
    "\n",
    "num_epochs = 10\n",
    "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love this phone', 'I hate spaghetti', 'Everything was cold', 'Everything was hot exactly as I wanted', 'Everything was green', 'the host seated us immediately', 'they gave us free chocolate cake', 'not sure about the wilted flowers on the table', 'only works when I stand on tippy toes', 'does not work when I stand on my head']\n",
      "\n",
      "Predictions for the user reviews:\n",
      "I love this phone\n",
      "[0.9959698] \n",
      "\n",
      "I hate spaghetti\n",
      "[0.04349113] \n",
      "\n",
      "Everything was cold\n",
      "[0.4546066] \n",
      "\n",
      "Everything was hot exactly as I wanted\n",
      "[0.877783] \n",
      "\n",
      "Everything was green\n",
      "[0.5545907] \n",
      "\n",
      "the host seated us immediately\n",
      "[0.75958633] \n",
      "\n",
      "they gave us free chocolate cake\n",
      "[0.76843816] \n",
      "\n",
      "not sure about the wilted flowers on the table\n",
      "[0.02256727] \n",
      "\n",
      "only works when I stand on tippy toes\n",
      "[0.9578408] \n",
      "\n",
      "does not work when I stand on my head\n",
      "[0.00701591] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict a review   \n",
    "user_reviews = ['I love this phone', 'I hate spaghetti', \n",
    "                'Everything was cold',\n",
    "                'Everything was hot exactly as I wanted', \n",
    "                'Everything was green', \n",
    "                'the host seated us immediately',\n",
    "                'they gave us free chocolate cake', \n",
    "                'not sure about the wilted flowers on the table',\n",
    "                'only works when I stand on tippy toes', \n",
    "                'does not work when I stand on my head']\n",
    "print(user_reviews) \n",
    "\n",
    "# Create the sequences\n",
    "padding_type = 'post'\n",
    "sample_sequences = tokenizer.texts_to_sequences(user_reviews)\n",
    "user_padded = pad_sequences(sample_sequences, padding = padding_type, maxlen = max_length)           \n",
    "\n",
    "print('\\nPredictions for the user reviews:')              \n",
    "classes = model.predict(user_padded)\n",
    "\n",
    "# The closer the class is to 1, the more positive the review is deemed to be\n",
    "for x in range(len(user_reviews)):\n",
    "  print(user_reviews[x])\n",
    "  print(classes[x], '\\n')\n",
    "\n",
    "# The predictions are mostly around 0.5, so they're not too good. But we should considerate that the reviews were confusing on\n",
    "# purpose sometimes, to check how the model did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Tweaking the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 50, 16)            8000      \n",
      "                                                                 \n",
      " global_average_pooling1d_1   (None, 16)               0         \n",
      " (GlobalAveragePooling1D)                                        \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 6)                 102       \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,109\n",
      "Trainable params: 8,109\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "50/50 [==============================] - 1s 4ms/step - loss: 0.6927 - accuracy: 0.5091 - val_loss: 0.6952 - val_accuracy: 0.4110\n",
      "Epoch 2/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6905 - accuracy: 0.5223 - val_loss: 0.6964 - val_accuracy: 0.4110\n",
      "Epoch 3/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6878 - accuracy: 0.5235 - val_loss: 0.6952 - val_accuracy: 0.4261\n",
      "Epoch 4/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6841 - accuracy: 0.5405 - val_loss: 0.6906 - val_accuracy: 0.4837\n",
      "Epoch 5/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6780 - accuracy: 0.5876 - val_loss: 0.6855 - val_accuracy: 0.5213\n",
      "Epoch 6/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6692 - accuracy: 0.6522 - val_loss: 0.6779 - val_accuracy: 0.5439\n",
      "Epoch 7/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6566 - accuracy: 0.6698 - val_loss: 0.6623 - val_accuracy: 0.6566\n",
      "Epoch 8/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6396 - accuracy: 0.7294 - val_loss: 0.6501 - val_accuracy: 0.6416\n",
      "Epoch 9/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.6162 - accuracy: 0.7433 - val_loss: 0.6298 - val_accuracy: 0.7243\n",
      "Epoch 10/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5892 - accuracy: 0.7784 - val_loss: 0.6150 - val_accuracy: 0.6892\n",
      "Epoch 11/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5596 - accuracy: 0.7884 - val_loss: 0.5992 - val_accuracy: 0.6942\n",
      "Epoch 12/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.5286 - accuracy: 0.8098 - val_loss: 0.5756 - val_accuracy: 0.7368\n",
      "Epoch 13/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4976 - accuracy: 0.8305 - val_loss: 0.5611 - val_accuracy: 0.7268\n",
      "Epoch 14/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4699 - accuracy: 0.8355 - val_loss: 0.5420 - val_accuracy: 0.7494\n",
      "Epoch 15/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.8393 - val_loss: 0.5320 - val_accuracy: 0.7419\n",
      "Epoch 16/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.4204 - accuracy: 0.8456 - val_loss: 0.5148 - val_accuracy: 0.7644\n",
      "Epoch 17/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.3995 - accuracy: 0.8506 - val_loss: 0.5022 - val_accuracy: 0.7820\n",
      "Epoch 18/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.3812 - accuracy: 0.8663 - val_loss: 0.5024 - val_accuracy: 0.7619\n",
      "Epoch 19/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.3638 - accuracy: 0.8632 - val_loss: 0.4893 - val_accuracy: 0.7845\n",
      "Epoch 20/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.3499 - accuracy: 0.8763 - val_loss: 0.4985 - val_accuracy: 0.7594\n",
      "Epoch 21/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.3353 - accuracy: 0.8719 - val_loss: 0.5031 - val_accuracy: 0.7444\n",
      "Epoch 22/30\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8726 - val_loss: 0.4835 - val_accuracy: 0.7694\n",
      "Epoch 23/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.3129 - accuracy: 0.8839 - val_loss: 0.4797 - val_accuracy: 0.7719\n",
      "Epoch 24/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.3027 - accuracy: 0.8820 - val_loss: 0.4894 - val_accuracy: 0.7669\n",
      "Epoch 25/30\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2936 - accuracy: 0.8876 - val_loss: 0.4938 - val_accuracy: 0.7519\n",
      "Epoch 26/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2850 - accuracy: 0.8870 - val_loss: 0.4785 - val_accuracy: 0.7719\n",
      "Epoch 27/30\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 0.2791 - accuracy: 0.8895 - val_loss: 0.5237 - val_accuracy: 0.7168\n",
      "Epoch 28/30\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2686 - accuracy: 0.8933 - val_loss: 0.4818 - val_accuracy: 0.7719\n",
      "Epoch 29/30\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2639 - accuracy: 0.8952 - val_loss: 0.5226 - val_accuracy: 0.7293\n",
      "Epoch 30/30\n",
      "50/50 [==============================] - 0s 3ms/step - loss: 0.2589 - accuracy: 0.8927 - val_loss: 0.4993 - val_accuracy: 0.7469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e85db0b790>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We define another model, but with different values for the parameters\n",
    "vocab_size = 500           # Maximun number of words. Before it was 1000\n",
    "embedding_dim = 16          # Maximun number of possible sentiments\n",
    "max_length = 50            # Maximun lenght of the sequences. Before it was 100\n",
    "trunc_type='post'           # Truncate the end of the sequences\n",
    "padding_type='post'         # Pad the end of the sequences\n",
    "oov_tok = \"<OOV>\"           # Out Of Vocabulary token\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences, maxlen = max_length, padding = padding_type, truncating = trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen = max_length, padding = padding_type, truncating = trunc_type)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length = max_length),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),      # Using GlobalAveragePooling() instead of flatten(). It's supposed to give better results.\n",
    "    tf.keras.layers.Dense(6, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
    "])\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])     \n",
    "# This are the parameters recommended by TF for NLP and tokenization models \n",
    "model.summary()\n",
    "\n",
    "num_epochs = 30         # 30 epochs instead of 10\n",
    "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I love this phone', 'I hate spaghetti', 'Everything was cold', 'Everything was hot exactly as I wanted', 'Everything was green', 'the host seated us immediately', 'they gave us free chocolate cake', 'not sure about the wilted flowers on the table', 'only works when I stand on tippy toes', 'does not work when I stand on my head']\n",
      "\n",
      "Predictions for the user reviews:\n",
      "I love this phone\n",
      "[0.91190165] \n",
      "\n",
      "I hate spaghetti\n",
      "[0.162869] \n",
      "\n",
      "Everything was cold\n",
      "[0.6069859] \n",
      "\n",
      "Everything was hot exactly as I wanted\n",
      "[0.4335965] \n",
      "\n",
      "Everything was green\n",
      "[0.6069859] \n",
      "\n",
      "the host seated us immediately\n",
      "[0.5828072] \n",
      "\n",
      "they gave us free chocolate cake\n",
      "[0.8504108] \n",
      "\n",
      "not sure about the wilted flowers on the table\n",
      "[0.09598145] \n",
      "\n",
      "only works when I stand on tippy toes\n",
      "[0.85347605] \n",
      "\n",
      "does not work when I stand on my head\n",
      "[0.02362505] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the model to predict a review. This is the same part of code repeated, to check results after tweaking.\n",
    "user_reviews = ['I love this phone', 'I hate spaghetti', \n",
    "                'Everything was cold',\n",
    "                'Everything was hot exactly as I wanted', \n",
    "                'Everything was green', \n",
    "                'the host seated us immediately',\n",
    "                'they gave us free chocolate cake', \n",
    "                'not sure about the wilted flowers on the table',\n",
    "                'only works when I stand on tippy toes', \n",
    "                'does not work when I stand on my head']\n",
    "print(user_reviews) \n",
    "\n",
    "# Create the sequences\n",
    "padding_type = 'post'\n",
    "sample_sequences = tokenizer.texts_to_sequences(user_reviews)\n",
    "user_padded = pad_sequences(sample_sequences, padding = padding_type, maxlen = max_length)           \n",
    "\n",
    "print('\\nPredictions for the user reviews:')              \n",
    "classes = model.predict(user_padded)\n",
    "\n",
    "# The closer the class is to 1, the more positive the review is deemed to be\n",
    "for x in range(len(user_reviews)):\n",
    "    print(user_reviews[x])\n",
    "    print(classes[x], '\\n')\n",
    "\n",
    "# Before, the predictions were mostly around 0.5 and we didn't consider them too good.\n",
    "# This time, after the tweaks, they are much better. For instance, the model recognises words like love or free, considering them as\n",
    "# positive words, and words like dont, not or hate, as bad words. \n",
    "# There are still some ambiguous reviews that get around 0.5 as the model doesn't recognise well the sentiment."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63f0348d2a946343dffa286e77d1d2673ee07cffce91d303b3699736551171f0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
