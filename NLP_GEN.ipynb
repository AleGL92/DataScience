{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING SONG LYRICS WITH NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing the lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to remove the punctuation and uppercases\n",
    "def tokenize_corpus(corpus, num_words = -1):\n",
    "    if num_words > -1:\n",
    "        tokenizer = Tokenizer(num_words = num_words)\n",
    "    else:\n",
    "        tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    return tokenizer\n",
    "\n",
    "def create_lyrics_corpus(dataset, field):\n",
    "    # Remove all other punctuation\n",
    "    dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
    "    # Make it lowercase\n",
    "    dataset[field] = dataset[field].str.lower()\n",
    "    # Make it one long string to split by line\n",
    "    lyrics = dataset[field].str.cat()\n",
    "    corpus = lyrics.split('\\n')\n",
    "    # Remove any trailing whitespace\n",
    "    for l in range(len(corpus)):\n",
    "        corpus[l] = corpus[l].rstrip()\n",
    "    # Remove any empty lines\n",
    "    corpus = [l for l in corpus if l != '']\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garci\\AppData\\Local\\Temp/ipykernel_15852/2596313655.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n"
     ]
    }
   ],
   "source": [
    "path = 'E:\\\\ALEJANDRO\\\\DATA_SCIENCE\\\\TENSORFLOW\\\\NLP_C9_10\\\\songdata.csv'\n",
    "# ds = pd.read_csv(path, dtype=str)[:10]      # Just 10 songs the first time, but 250 if we want to get better results.\n",
    "ds = pd.read_csv(path, dtype=str)[:250]\n",
    "# Create the corpus using the 'text' column containing lyrics\n",
    "corpus = create_lyrics_corpus(ds, 'text')\n",
    "# Tokenize the corpus\n",
    "tokenizer = tokenize_corpus(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# print(tokenizer.word_index)\n",
    "print(total_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Creating sequences and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0 111  66]\n",
      " [  0   0   0 ... 111  66  86]\n",
      " [  0   0   0 ...  66  86 206]\n",
      " ...\n",
      " [  0   0   0 ... 567 137  93]\n",
      " [  0   0   0 ... 137  93  20]\n",
      " [  0   0   0 ...   0 952 151]]\n",
      "Separator\n",
      "[[  0   0   0 ...   0   0 111]\n",
      " [  0   0   0 ...   0 111  66]\n",
      " [  0   0   0 ... 111  66  86]\n",
      " ...\n",
      " [  0   0   0 ...  15 567 137]\n",
      " [  0   0   0 ... 567 137  93]\n",
      " [  0   0   0 ...   0   0 952]]\n",
      "Separator\n",
      "[ 66  86 206 ...  93  20 151]\n"
     ]
    }
   ],
   "source": [
    "# An N-gram can be considered as a sequence of N words, by that notion, a 2-gram (or bigram) is a two-word sequence of words like “please turn”, “turn your”, or \n",
    "# ”your homework”, and a 3-gram (or trigram) is a three-word sequence of words like “please turn your”, or “turn your homework”. We assign probabilities to an N-gram, \n",
    "# so that we can predict better wich words are coming next. It's more probable than we have homework, after 'turn your', than other words. Sometimes it's even like a \n",
    "# single word, like high school or San Francisco, which have a very high probability to go together.\n",
    "# So we could say The N-gram is a model that predicts the next word, based on the N-1, N-2, ..., N-n words.\n",
    "\n",
    "sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tsequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences for equal input length \n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "sequences = np.array(pad_sequences(sequences, maxlen = max_sequence_len, padding = 'pre'))\n",
    "print(sequences)\n",
    "print('Separator')\n",
    "\n",
    "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
    "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
    "# The -1 position means the last one.\n",
    "print(input_sequences)\n",
    "print('Separator')\n",
    "print(labels)\n",
    "# One-hot encode the labels. Now each labels becomes one hot encoded\n",
    "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes = total_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "158\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0 111  66  86 206  29\n",
      "   4]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0  111   66\n",
      "   86  206   29    4 1196]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Check out how some of our data is being stored\n",
    "# The Tokenizer has just a single index per word\n",
    "print(tokenizer.word_index['know'])\n",
    "print(tokenizer.word_index['feeling'])\n",
    "# Input sequences will have multiple indexes\n",
    "print(input_sequences[5])\n",
    "print(input_sequences[6])\n",
    "# And the one hot labels will be as long as the full spread of tokenized words\n",
    "print(one_hot_labels[5])\n",
    "print(one_hot_labels[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Training a text generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1557/1557 [==============================] - 21s 11ms/step - loss: 6.3133 - accuracy: 0.0445\n",
      "Epoch 2/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 5.9585 - accuracy: 0.0482\n",
      "Epoch 3/200\n",
      "1557/1557 [==============================] - 16s 11ms/step - loss: 5.7032 - accuracy: 0.0655\n",
      "Epoch 4/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 5.4926 - accuracy: 0.0864\n",
      "Epoch 5/200\n",
      "1557/1557 [==============================] - 16s 11ms/step - loss: 5.2858 - accuracy: 0.1122\n",
      "Epoch 6/200\n",
      "1557/1557 [==============================] - 16s 10ms/step - loss: 5.0863 - accuracy: 0.1304\n",
      "Epoch 7/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 4.9037 - accuracy: 0.1458\n",
      "Epoch 8/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 4.7429 - accuracy: 0.1595\n",
      "Epoch 9/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 4.6015 - accuracy: 0.1732\n",
      "Epoch 10/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 4.4738 - accuracy: 0.1882\n",
      "Epoch 11/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 4.3558 - accuracy: 0.2014\n",
      "Epoch 12/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 4.2484 - accuracy: 0.2152\n",
      "Epoch 13/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 4.1478 - accuracy: 0.2264\n",
      "Epoch 14/200\n",
      "1557/1557 [==============================] - 22s 14ms/step - loss: 4.0577 - accuracy: 0.2368\n",
      "Epoch 15/200\n",
      "1557/1557 [==============================] - 18s 11ms/step - loss: 3.9704 - accuracy: 0.2484\n",
      "Epoch 16/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 3.8922 - accuracy: 0.2594\n",
      "Epoch 17/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 3.8189 - accuracy: 0.2675\n",
      "Epoch 18/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 3.7507 - accuracy: 0.2777\n",
      "Epoch 19/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 3.6831 - accuracy: 0.2867\n",
      "Epoch 20/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 3.6176 - accuracy: 0.2940\n",
      "Epoch 21/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 3.5612 - accuracy: 0.3031\n",
      "Epoch 22/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 3.5022 - accuracy: 0.3104\n",
      "Epoch 23/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 3.4497 - accuracy: 0.3189\n",
      "Epoch 24/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 3.4049 - accuracy: 0.3254\n",
      "Epoch 25/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 3.3529 - accuracy: 0.3326\n",
      "Epoch 26/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 3.3158 - accuracy: 0.3389\n",
      "Epoch 27/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 3.2671 - accuracy: 0.3476\n",
      "Epoch 28/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 3.2282 - accuracy: 0.3533\n",
      "Epoch 29/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 3.1795 - accuracy: 0.3614\n",
      "Epoch 30/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 3.1420 - accuracy: 0.3675\n",
      "Epoch 31/200\n",
      "1557/1557 [==============================] - 21s 13ms/step - loss: 3.1082 - accuracy: 0.3700\n",
      "Epoch 32/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 3.0704 - accuracy: 0.3765\n",
      "Epoch 33/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 3.0370 - accuracy: 0.3816\n",
      "Epoch 34/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 3.0021 - accuracy: 0.3871\n",
      "Epoch 35/200\n",
      "1557/1557 [==============================] - 18s 11ms/step - loss: 2.9701 - accuracy: 0.3924\n",
      "Epoch 36/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.9356 - accuracy: 0.3977\n",
      "Epoch 37/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 2.9125 - accuracy: 0.4024\n",
      "Epoch 38/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.8796 - accuracy: 0.4048\n",
      "Epoch 39/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.8570 - accuracy: 0.4095\n",
      "Epoch 40/200\n",
      "1557/1557 [==============================] - 20s 13ms/step - loss: 2.8209 - accuracy: 0.4152\n",
      "Epoch 41/200\n",
      "1557/1557 [==============================] - 20s 13ms/step - loss: 2.8035 - accuracy: 0.4187\n",
      "Epoch 42/200\n",
      "1557/1557 [==============================] - 20s 13ms/step - loss: 2.7736 - accuracy: 0.4244\n",
      "Epoch 43/200\n",
      "1557/1557 [==============================] - 21s 14ms/step - loss: 2.7533 - accuracy: 0.4270\n",
      "Epoch 44/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.7290 - accuracy: 0.4316\n",
      "Epoch 45/200\n",
      "1557/1557 [==============================] - 21s 14ms/step - loss: 2.7077 - accuracy: 0.4358\n",
      "Epoch 46/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.6841 - accuracy: 0.4390\n",
      "Epoch 47/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.6612 - accuracy: 0.4444\n",
      "Epoch 48/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.6453 - accuracy: 0.4461\n",
      "Epoch 49/200\n",
      "1557/1557 [==============================] - 20s 13ms/step - loss: 2.6287 - accuracy: 0.4490\n",
      "Epoch 50/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.5987 - accuracy: 0.4537\n",
      "Epoch 51/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.5815 - accuracy: 0.4568\n",
      "Epoch 52/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.5649 - accuracy: 0.4589\n",
      "Epoch 53/200\n",
      "1557/1557 [==============================] - 20s 13ms/step - loss: 2.5465 - accuracy: 0.4629\n",
      "Epoch 54/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.5522 - accuracy: 0.4620\n",
      "Epoch 55/200\n",
      "1557/1557 [==============================] - 20s 13ms/step - loss: 2.5080 - accuracy: 0.4703\n",
      "Epoch 56/200\n",
      "1557/1557 [==============================] - 19s 13ms/step - loss: 2.4975 - accuracy: 0.4733\n",
      "Epoch 57/200\n",
      "1557/1557 [==============================] - 20s 13ms/step - loss: 2.4753 - accuracy: 0.4752\n",
      "Epoch 58/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.4940 - accuracy: 0.4723\n",
      "Epoch 59/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.4542 - accuracy: 0.4791\n",
      "Epoch 60/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.4410 - accuracy: 0.4812\n",
      "Epoch 61/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.4254 - accuracy: 0.4863\n",
      "Epoch 62/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.3996 - accuracy: 0.4887\n",
      "Epoch 63/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.3930 - accuracy: 0.4907\n",
      "Epoch 64/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.3768 - accuracy: 0.4942\n",
      "Epoch 65/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.3794 - accuracy: 0.4923\n",
      "Epoch 66/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.3539 - accuracy: 0.4980\n",
      "Epoch 67/200\n",
      "1557/1557 [==============================] - 20s 13ms/step - loss: 2.3435 - accuracy: 0.5017\n",
      "Epoch 68/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.3357 - accuracy: 0.5020\n",
      "Epoch 69/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.3402 - accuracy: 0.5006\n",
      "Epoch 70/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.3079 - accuracy: 0.5068\n",
      "Epoch 71/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.3065 - accuracy: 0.5066\n",
      "Epoch 72/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.2862 - accuracy: 0.5097\n",
      "Epoch 73/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.2581 - accuracy: 0.5155\n",
      "Epoch 74/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.2545 - accuracy: 0.5154\n",
      "Epoch 75/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.2451 - accuracy: 0.5174\n",
      "Epoch 76/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.2421 - accuracy: 0.5184\n",
      "Epoch 77/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.2269 - accuracy: 0.5194\n",
      "Epoch 78/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.2202 - accuracy: 0.5225\n",
      "Epoch 79/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.2145 - accuracy: 0.5224\n",
      "Epoch 80/200\n",
      "1557/1557 [==============================] - 20s 13ms/step - loss: 2.1943 - accuracy: 0.5281\n",
      "Epoch 81/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.1842 - accuracy: 0.5300\n",
      "Epoch 82/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.1823 - accuracy: 0.5280\n",
      "Epoch 83/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.1755 - accuracy: 0.5310\n",
      "Epoch 84/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.1592 - accuracy: 0.5336\n",
      "Epoch 85/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.1492 - accuracy: 0.5361\n",
      "Epoch 86/200\n",
      "1557/1557 [==============================] - 20s 13ms/step - loss: 2.1459 - accuracy: 0.5368\n",
      "Epoch 87/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.1486 - accuracy: 0.5338\n",
      "Epoch 88/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.1383 - accuracy: 0.5361\n",
      "Epoch 89/200\n",
      "1557/1557 [==============================] - 20s 13ms/step - loss: 2.1127 - accuracy: 0.5409\n",
      "Epoch 90/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 2.1141 - accuracy: 0.5425\n",
      "Epoch 91/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 2.1366 - accuracy: 0.5354\n",
      "Epoch 92/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 2.0977 - accuracy: 0.5444\n",
      "Epoch 93/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 2.0885 - accuracy: 0.5451\n",
      "Epoch 94/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 2.0919 - accuracy: 0.5430\n",
      "Epoch 95/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 2.0902 - accuracy: 0.5458\n",
      "Epoch 96/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 2.0582 - accuracy: 0.5530\n",
      "Epoch 97/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 2.0534 - accuracy: 0.5543\n",
      "Epoch 98/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.0399 - accuracy: 0.5555\n",
      "Epoch 99/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.0505 - accuracy: 0.5533\n",
      "Epoch 100/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.0255 - accuracy: 0.5582\n",
      "Epoch 101/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.0452 - accuracy: 0.5542\n",
      "Epoch 102/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.0152 - accuracy: 0.5593\n",
      "Epoch 103/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.0144 - accuracy: 0.5599\n",
      "Epoch 104/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 2.0072 - accuracy: 0.5609\n",
      "Epoch 105/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.9971 - accuracy: 0.5647\n",
      "Epoch 106/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.9964 - accuracy: 0.5627\n",
      "Epoch 107/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.9863 - accuracy: 0.5648\n",
      "Epoch 108/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.9794 - accuracy: 0.5674\n",
      "Epoch 109/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.9786 - accuracy: 0.5672\n",
      "Epoch 110/200\n",
      "1557/1557 [==============================] - 18s 11ms/step - loss: 1.9734 - accuracy: 0.5692\n",
      "Epoch 111/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.9599 - accuracy: 0.5718\n",
      "Epoch 112/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 1.9564 - accuracy: 0.5713\n",
      "Epoch 113/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.9511 - accuracy: 0.5721\n",
      "Epoch 114/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.9604 - accuracy: 0.5715\n",
      "Epoch 115/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.9338 - accuracy: 0.5762\n",
      "Epoch 116/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.9365 - accuracy: 0.5738\n",
      "Epoch 117/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 1.9214 - accuracy: 0.5775\n",
      "Epoch 118/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 1.9227 - accuracy: 0.5785\n",
      "Epoch 119/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 1.9249 - accuracy: 0.5778\n",
      "Epoch 120/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 1.9043 - accuracy: 0.5820\n",
      "Epoch 121/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.9022 - accuracy: 0.5821\n",
      "Epoch 122/200\n",
      "1557/1557 [==============================] - 19s 12ms/step - loss: 1.9045 - accuracy: 0.5814\n",
      "Epoch 123/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.8919 - accuracy: 0.5830\n",
      "Epoch 124/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.8931 - accuracy: 0.5825\n",
      "Epoch 125/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.8938 - accuracy: 0.5824\n",
      "Epoch 126/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.8906 - accuracy: 0.5845\n",
      "Epoch 127/200\n",
      "1557/1557 [==============================] - 18s 11ms/step - loss: 1.8739 - accuracy: 0.5872\n",
      "Epoch 128/200\n",
      "1557/1557 [==============================] - 18s 11ms/step - loss: 1.8760 - accuracy: 0.5857\n",
      "Epoch 129/200\n",
      "1557/1557 [==============================] - 18s 11ms/step - loss: 1.8613 - accuracy: 0.5885\n",
      "Epoch 130/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.8650 - accuracy: 0.5894\n",
      "Epoch 131/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.8744 - accuracy: 0.5842\n",
      "Epoch 132/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.8587 - accuracy: 0.5897\n",
      "Epoch 133/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.8523 - accuracy: 0.5906\n",
      "Epoch 134/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.8473 - accuracy: 0.5920\n",
      "Epoch 135/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.8418 - accuracy: 0.5929\n",
      "Epoch 136/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.8609 - accuracy: 0.5886\n",
      "Epoch 137/200\n",
      "1557/1557 [==============================] - 18s 11ms/step - loss: 1.8272 - accuracy: 0.5974\n",
      "Epoch 138/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.8210 - accuracy: 0.5968\n",
      "Epoch 139/200\n",
      "1557/1557 [==============================] - 18s 12ms/step - loss: 1.8342 - accuracy: 0.5936\n",
      "Epoch 140/200\n",
      "1557/1557 [==============================] - 18s 11ms/step - loss: 1.8326 - accuracy: 0.5951\n",
      "Epoch 141/200\n",
      "1557/1557 [==============================] - 18s 11ms/step - loss: 1.8090 - accuracy: 0.5990\n",
      "Epoch 142/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.8087 - accuracy: 0.5993\n",
      "Epoch 143/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.8515 - accuracy: 0.5893\n",
      "Epoch 144/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7983 - accuracy: 0.6007\n",
      "Epoch 145/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.8169 - accuracy: 0.5964\n",
      "Epoch 146/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.8019 - accuracy: 0.6018\n",
      "Epoch 147/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7822 - accuracy: 0.6047\n",
      "Epoch 148/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7915 - accuracy: 0.6034\n",
      "Epoch 149/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7899 - accuracy: 0.6035\n",
      "Epoch 150/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7949 - accuracy: 0.6031\n",
      "Epoch 151/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7802 - accuracy: 0.6033\n",
      "Epoch 152/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7829 - accuracy: 0.6033\n",
      "Epoch 153/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7876 - accuracy: 0.6033\n",
      "Epoch 154/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7792 - accuracy: 0.6034\n",
      "Epoch 155/200\n",
      "1557/1557 [==============================] - 18s 11ms/step - loss: 1.7699 - accuracy: 0.6070\n",
      "Epoch 156/200\n",
      "1557/1557 [==============================] - 18s 11ms/step - loss: 1.7747 - accuracy: 0.6036\n",
      "Epoch 157/200\n",
      "1557/1557 [==============================] - 18s 11ms/step - loss: 1.7637 - accuracy: 0.6077\n",
      "Epoch 158/200\n",
      "1557/1557 [==============================] - 18s 11ms/step - loss: 1.7567 - accuracy: 0.6089\n",
      "Epoch 159/200\n",
      "1557/1557 [==============================] - 18s 11ms/step - loss: 1.7625 - accuracy: 0.6078\n",
      "Epoch 160/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7478 - accuracy: 0.6107\n",
      "Epoch 161/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7637 - accuracy: 0.6068\n",
      "Epoch 162/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7461 - accuracy: 0.6092\n",
      "Epoch 163/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7399 - accuracy: 0.6124\n",
      "Epoch 164/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7393 - accuracy: 0.6115\n",
      "Epoch 165/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7378 - accuracy: 0.6113\n",
      "Epoch 166/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7487 - accuracy: 0.6096\n",
      "Epoch 167/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7450 - accuracy: 0.6099\n",
      "Epoch 168/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7291 - accuracy: 0.6143\n",
      "Epoch 169/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7189 - accuracy: 0.6149\n",
      "Epoch 170/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7141 - accuracy: 0.6159\n",
      "Epoch 171/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7149 - accuracy: 0.6175\n",
      "Epoch 172/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7060 - accuracy: 0.6181\n",
      "Epoch 173/200\n",
      "1557/1557 [==============================] - 18s 11ms/step - loss: 1.7410 - accuracy: 0.6102\n",
      "Epoch 174/200\n",
      "1557/1557 [==============================] - 18s 11ms/step - loss: 1.7249 - accuracy: 0.6132\n",
      "Epoch 175/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6896 - accuracy: 0.6221\n",
      "Epoch 176/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7053 - accuracy: 0.6197\n",
      "Epoch 177/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6974 - accuracy: 0.6189\n",
      "Epoch 178/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7025 - accuracy: 0.6177\n",
      "Epoch 179/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6966 - accuracy: 0.6184\n",
      "Epoch 180/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6892 - accuracy: 0.6200\n",
      "Epoch 181/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7278 - accuracy: 0.6112\n",
      "Epoch 182/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6788 - accuracy: 0.6223\n",
      "Epoch 183/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6653 - accuracy: 0.6261\n",
      "Epoch 184/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6936 - accuracy: 0.6195\n",
      "Epoch 185/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6908 - accuracy: 0.6200\n",
      "Epoch 186/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6687 - accuracy: 0.6267\n",
      "Epoch 187/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6817 - accuracy: 0.6221\n",
      "Epoch 188/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6780 - accuracy: 0.6220\n",
      "Epoch 189/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6675 - accuracy: 0.6250\n",
      "Epoch 190/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6573 - accuracy: 0.6271\n",
      "Epoch 191/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6671 - accuracy: 0.6252\n",
      "Epoch 192/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6721 - accuracy: 0.6234\n",
      "Epoch 193/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6686 - accuracy: 0.6239\n",
      "Epoch 194/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6720 - accuracy: 0.6245\n",
      "Epoch 195/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6764 - accuracy: 0.6227\n",
      "Epoch 196/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6420 - accuracy: 0.6306\n",
      "Epoch 197/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6576 - accuracy: 0.6260\n",
      "Epoch 198/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6752 - accuracy: 0.6229\n",
      "Epoch 199/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.7068 - accuracy: 0.6160\n",
      "Epoch 200/200\n",
      "1557/1557 [==============================] - 17s 11ms/step - loss: 1.6542 - accuracy: 0.6280\n"
     ]
    }
   ],
   "source": [
    "# We'll use a Recurrent Neuronal Network for our model. It will be very similar to the sentiment models of the previous codes, but with the some small changes.\n",
    "# It's necessary to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now \n",
    "# there are hundreds of categories. Also we need to use much more epochs than before, to get acceptable results.\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(20)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.show()\n",
    "\n",
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im feeling chills me so long light me out that with you go wrong i have to go by moment ha of somebody spine embers posses bit stops married cunning owe owe paris forgot tame patronizin patronizin cut mouth fault fault fault fault fault glass pimple refined teasing plain losing seats forgot havent piper fuck fault fault fault freeze glass think covers think renee someday calm oasis havent gate a hangup tears beside bluest loveland firm shining up yeah with yeah forever trapped in cost my mind baby now stop me and see for life for you tonight honey i dont know how\n"
     ]
    }
   ],
   "source": [
    "# Finally, we generate some lyrics\n",
    "seed_text = \"im feeling chills\"\n",
    "next_words = 100\n",
    "\n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)\n",
    "# As we can see, the results are not too good. There are many repeated words, and in general, \n",
    "# the text doesn't make much sense. We'll need to improve our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.6656761e-10 3.5960533e-02 1.5181344e-03 ... 0.0000000e+00 0.0000000e+00\n",
      " 2.2044516e-27]\n",
      "[3.0930106e-11 1.5513013e-01 7.8138309e-03 ... 1.5636746e-34 1.0656852e-37\n",
      " 4.9519472e-23]\n",
      "[7.6897093e-11 1.3235353e-03 3.7439402e-02 ... 2.8827946e-22 3.4438809e-19\n",
      " 4.2659964e-12]\n",
      "[1.8836508e-13 8.4417668e-04 3.8056682e-05 ... 1.3158033e-24 3.0762241e-22\n",
      " 2.2879774e-35]\n",
      "[3.1758218e-11 3.7664941e-03 1.1959235e-06 ... 0.0000000e+00 0.0000000e+00\n",
      " 3.0604291e-28]\n",
      "[2.8443637e-13 2.5756444e-06 3.6176818e-05 ... 1.0508552e-24 4.6526455e-23\n",
      " 1.2778533e-21]\n",
      "[2.5059403e-13 7.4224189e-08 4.3866469e-04 ... 1.4186465e-33 8.3254796e-36\n",
      " 4.1861590e-34]\n",
      "[5.0780903e-14 7.4979803e-06 7.7214936e-04 ... 8.2035200e-32 5.9753934e-24\n",
      " 1.3166172e-23]\n",
      "[1.05968574e-10 4.94560366e-03 3.69987031e-03 ... 1.30136530e-23\n",
      " 8.71969682e-26 1.40602799e-11]\n",
      "[1.3328768e-11 1.4377538e-04 6.0659813e-06 ... 1.2754466e-31 3.1403716e-35\n",
      " 4.8783963e-35]\n",
      "[2.3185120e-10 9.8689422e-03 3.3142071e-05 ... 2.1634976e-34 0.0000000e+00\n",
      " 1.1206039e-24]\n",
      "[8.1118762e-10 7.0788554e-04 2.8259228e-03 ... 2.0867631e-24 4.3639181e-21\n",
      " 1.8842591e-20]\n",
      "[5.78877120e-11 1.12682686e-03 1.07328065e-04 ... 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00]\n",
      "[2.5704211e-10 1.1323266e-02 1.2723098e-03 ... 0.0000000e+00 0.0000000e+00\n",
      " 4.4753548e-25]\n",
      "[2.92778307e-13 3.12616539e-05 7.99729150e-07 ... 6.63084344e-38\n",
      " 4.23846621e-34 1.01415565e-26]\n",
      "[4.0879647e-11 4.5985691e-02 3.0790923e-06 ... 2.7737854e-33 0.0000000e+00\n",
      " 1.7899715e-22]\n",
      "[1.5974662e-10 7.3440766e-01 1.7480363e-05 ... 4.1932586e-36 2.2902931e-38\n",
      " 4.6063952e-21]\n",
      "[2.6795671e-12 9.6880322e-05 6.9743153e-05 ... 2.5338913e-09 1.6286958e-09\n",
      " 2.6504466e-08]\n",
      "[2.7331531e-12 7.2865128e-06 2.2174183e-05 ... 1.3756892e-28 8.5979257e-34\n",
      " 2.7211457e-21]\n",
      "[1.3916515e-11 1.5590712e-03 4.3698986e-08 ... 3.1517576e-29 2.1494643e-32\n",
      " 4.6960071e-25]\n",
      "[1.00773424e-10 2.88801491e-01 4.62129947e-06 ... 1.10050414e-37\n",
      " 0.00000000e+00 1.34885386e-23]\n",
      "[2.0149955e-09 5.9635378e-05 1.3116042e-05 ... 2.1150814e-07 7.0278566e-07\n",
      " 1.1458741e-05]\n",
      "[1.8844959e-10 3.5714008e-06 2.3405419e-05 ... 4.2008359e-12 7.2083985e-11\n",
      " 1.7023805e-13]\n",
      "[2.2524022e-10 4.7952586e-04 9.5137904e-05 ... 5.8389930e-20 2.5614007e-33\n",
      " 8.9473590e-27]\n",
      "[2.5099053e-10 9.3086964e-01 6.0878381e-07 ... 2.9300359e-18 2.2481634e-27\n",
      " 4.8070278e-17]\n",
      "[1.9985558e-08 1.0185090e-04 5.8148737e-05 ... 2.2040700e-07 7.7416262e-10\n",
      " 1.3136531e-10]\n",
      "[8.5299690e-10 1.0083860e-04 1.2301733e-03 ... 9.4789086e-11 1.6311305e-18\n",
      " 9.4578749e-21]\n",
      "[3.6660806e-11 3.7613777e-06 1.3946021e-06 ... 1.5473151e-14 1.0315198e-12\n",
      " 8.2056255e-29]\n",
      "[2.5621255e-10 9.7702362e-04 3.6555318e-07 ... 2.1962354e-13 9.9945110e-09\n",
      " 3.0758667e-27]\n",
      "[1.6519299e-09 4.0850631e-04 6.4734620e-04 ... 1.8153374e-22 8.5124793e-21\n",
      " 3.3192924e-24]\n",
      "[5.76126027e-08 1.07011976e-04 4.15508810e-04 ... 4.99905928e-15\n",
      " 1.04938733e-16 2.92573136e-22]\n",
      "[2.7255538e-09 6.8757981e-05 5.0005889e-03 ... 1.3325923e-07 5.1629645e-08\n",
      " 4.4144517e-13]\n",
      "[2.2270645e-10 1.8783357e-02 8.8882163e-02 ... 3.0716980e-37 0.0000000e+00\n",
      " 1.4863200e-33]\n",
      "[2.6868215e-12 5.6896359e-04 3.4913360e-03 ... 4.0041643e-26 4.6680212e-26\n",
      " 1.0185126e-18]\n",
      "[3.2175190e-11 5.9419819e-03 3.6114225e-01 ... 5.1056249e-27 1.8209830e-32\n",
      " 3.1168241e-23]\n",
      "[1.7018936e-10 4.4062044e-03 8.7935198e-04 ... 3.9077484e-15 5.8050208e-18\n",
      " 3.2940897e-11]\n",
      "[1.4669985e-08 2.2663138e-04 7.4794283e-04 ... 4.1447736e-15 3.3272029e-06\n",
      " 3.0682441e-22]\n",
      "[1.8517200e-11 1.3120056e-03 4.9137915e-03 ... 7.6122268e-30 3.1156770e-35\n",
      " 9.2712345e-26]\n",
      "[2.88065616e-09 1.76891845e-05 7.65850826e-04 ... 1.15774401e-09\n",
      " 8.51828996e-10 1.03522045e-10]\n",
      "[7.8915062e-11 2.9377945e-04 1.6238677e-05 ... 1.5987367e-23 3.6380492e-33\n",
      " 6.7771545e-23]\n",
      "[1.0678084e-10 6.7828395e-03 1.4985422e-06 ... 6.9002993e-19 3.1212723e-20\n",
      " 7.5204360e-27]\n",
      "[6.6684473e-13 1.2451540e-07 1.2007141e-07 ... 1.7124445e-18 1.0576261e-17\n",
      " 4.9997448e-32]\n",
      "[8.7879470e-10 3.1050669e-02 1.5371246e-05 ... 7.4734550e-29 1.1568575e-29\n",
      " 3.9256254e-34]\n",
      "[1.5629527e-10 3.4258966e-04 2.9414817e-08 ... 3.0680734e-17 6.1610534e-15\n",
      " 2.6021978e-35]\n",
      "[2.2115837e-11 4.2333022e-06 1.0048179e-04 ... 5.8682881e-20 3.1004244e-21\n",
      " 8.1368581e-24]\n",
      "[4.2082839e-09 1.7051494e-03 1.4483703e-05 ... 3.6770600e-17 8.5699484e-27\n",
      " 1.9279387e-28]\n",
      "[5.9149278e-12 2.1752615e-09 4.1132590e-11 ... 2.9000714e-16 6.1526225e-25\n",
      " 1.2602815e-30]\n",
      "[8.5638815e-11 3.1272966e-06 4.7776348e-06 ... 8.7756189e-22 6.1528610e-21\n",
      " 4.2745139e-22]\n",
      "[2.9255244e-11 1.0051344e-03 2.9500976e-02 ... 4.1415432e-27 4.1460598e-29\n",
      " 6.0547724e-30]\n",
      "[5.6918713e-11 8.4255633e-05 1.9550034e-05 ... 0.0000000e+00 0.0000000e+00\n",
      " 2.2203450e-35]\n",
      "[4.3062318e-10 2.8593469e-04 5.4827228e-07 ... 5.0916040e-18 1.8642155e-17\n",
      " 6.4992943e-29]\n",
      "[1.0264737e-08 4.3338467e-05 5.6831464e-05 ... 1.7934768e-24 8.5690142e-29\n",
      " 2.2933544e-27]\n",
      "[2.7324615e-11 4.8912246e-02 1.5735252e-05 ... 2.4160065e-37 0.0000000e+00\n",
      " 9.9743733e-31]\n",
      "[2.4597510e-10 2.4101450e-08 5.4022571e-06 ... 8.1005506e-17 2.2811529e-19\n",
      " 9.3991079e-15]\n",
      "[1.3115121e-09 3.6880045e-04 5.9444824e-04 ... 6.3805433e-18 2.2388318e-19\n",
      " 6.3471068e-28]\n",
      "[2.5425355e-09 6.1319958e-02 2.2558672e-06 ... 1.6368853e-20 1.9411122e-23\n",
      " 5.3148789e-13]\n",
      "[7.2985817e-10 5.0587062e-04 4.6359308e-08 ... 7.9313220e-17 6.9145857e-22\n",
      " 9.2011247e-25]\n",
      "[5.7850785e-10 1.6059324e-03 7.5741955e-05 ... 2.0944782e-28 7.4675170e-23\n",
      " 4.6598892e-34]\n",
      "[1.7620502e-09 1.2805024e-01 3.9131681e-08 ... 3.2262760e-17 2.6262209e-22\n",
      " 4.9498947e-28]\n",
      "[2.8486969e-08 3.4446351e-04 3.2524335e-05 ... 1.4451027e-06 5.1756075e-07\n",
      " 1.1032992e-06]\n",
      "[3.6943039e-08 1.1583058e-02 3.8857985e-04 ... 2.9806419e-24 3.5150252e-19\n",
      " 5.4383325e-31]\n",
      "[2.94751085e-10 1.25084072e-04 1.13765445e-05 ... 1.90195276e-25\n",
      " 1.85889645e-22 3.33640318e-24]\n",
      "[6.8026024e-10 4.0126275e-04 6.2206213e-06 ... 5.0352595e-17 1.9316560e-16\n",
      " 2.2597629e-18]\n",
      "[6.4371121e-13 8.4768259e-04 1.8531425e-09 ... 7.0001726e-24 8.7435400e-19\n",
      " 5.0497078e-27]\n",
      "[1.2260280e-10 9.2561645e-03 4.7151370e-06 ... 9.4739484e-20 8.8771941e-21\n",
      " 1.8969542e-14]\n",
      "[2.7905725e-10 1.6801556e-03 7.4935511e-02 ... 5.0559698e-27 1.0024443e-32\n",
      " 5.9542464e-22]\n",
      "[9.1296803e-10 1.2408258e-05 4.6107904e-03 ... 1.7440351e-11 1.4566254e-12\n",
      " 2.0723134e-19]\n",
      "[1.09537164e-10 1.49080774e-03 1.41112923e-05 ... 9.67007198e-29\n",
      " 2.19564892e-27 2.93909624e-30]\n",
      "[5.9931733e-11 5.7406141e-04 1.2623572e-02 ... 2.4680232e-34 0.0000000e+00\n",
      " 4.1675384e-24]\n",
      "[3.7031857e-11 2.9758096e-03 5.9562262e-07 ... 5.3178909e-33 2.7456477e-32\n",
      " 4.8278784e-21]\n",
      "[1.3794105e-10 4.6214368e-02 3.3081695e-04 ... 2.6783488e-31 0.0000000e+00\n",
      " 1.5618938e-21]\n",
      "[3.0940851e-11 1.1187772e-01 1.3574312e-03 ... 1.2380314e-33 0.0000000e+00\n",
      " 6.1703589e-20]\n",
      "[7.6339794e-12 9.5818961e-01 4.0029026e-06 ... 0.0000000e+00 0.0000000e+00\n",
      " 6.6570308e-26]\n",
      "[3.7149266e-11 1.0588572e-05 1.6591707e-03 ... 6.5845257e-10 6.4870487e-12\n",
      " 4.5761470e-10]\n",
      "[2.2127815e-11 7.7020304e-05 1.7894954e-06 ... 3.9501470e-29 2.0362765e-25\n",
      " 4.5130736e-27]\n",
      "[1.5169743e-09 5.4444450e-01 6.1765131e-06 ... 3.4083179e-25 9.1218669e-24\n",
      " 4.0054172e-28]\n",
      "[8.3417565e-11 8.2558372e-06 2.5759525e-07 ... 1.7152690e-05 1.9186479e-09\n",
      " 2.5330873e-14]\n",
      "[8.7784979e-12 6.5404191e-07 5.0332762e-08 ... 6.8850497e-32 6.0344452e-36\n",
      " 1.9513828e-31]\n",
      "[3.0980710e-10 4.3861876e-04 3.8977231e-07 ... 9.3829359e-20 2.3394142e-27\n",
      " 9.6809720e-34]\n",
      "[2.3058755e-10 3.0697312e-03 6.9588628e-09 ... 2.3617281e-14 4.2730655e-17\n",
      " 1.3531765e-34]\n",
      "[1.0349382e-11 1.6373892e-04 2.7012597e-05 ... 1.8167478e-14 4.1381858e-22\n",
      " 9.0002133e-26]\n",
      "[3.2283173e-10 6.4028784e-05 2.2301902e-01 ... 5.7281189e-27 0.0000000e+00\n",
      " 7.0591208e-30]\n",
      "[2.2656665e-09 3.2109834e-05 1.8801033e-01 ... 1.3255410e-23 2.4810415e-38\n",
      " 8.6154642e-31]\n",
      "[2.2252011e-10 2.9935092e-07 4.4104774e-04 ... 3.5646191e-11 2.4210252e-12\n",
      " 2.6216605e-19]\n",
      "[9.7450381e-10 9.2895149e-05 5.2463721e-02 ... 9.8289742e-27 1.1891547e-36\n",
      " 2.5189551e-31]\n",
      "[6.0801403e-10 5.3583367e-06 1.0237167e-05 ... 1.4908825e-19 7.2372100e-28\n",
      " 3.7621585e-27]\n",
      "[4.1194750e-09 1.5324857e-03 4.3393064e-02 ... 1.7700454e-31 8.0248772e-35\n",
      " 2.9301641e-27]\n",
      "[9.3749843e-09 9.7200173e-05 9.1339462e-02 ... 7.2900854e-35 1.4452992e-36\n",
      " 6.6493495e-37]\n",
      "[3.4688488e-09 6.1305868e-04 6.5336108e-02 ... 1.7676567e-21 1.5066827e-36\n",
      " 3.7210206e-20]\n",
      "[1.0075573e-12 1.0475097e-05 2.2690841e-05 ... 2.6524552e-30 2.4519438e-28\n",
      " 1.3983676e-33]\n",
      "[2.5583803e-18 1.3623575e-11 1.5285748e-07 ... 5.7396992e-34 4.9839874e-31\n",
      " 1.6893523e-36]\n",
      "[2.2279848e-11 7.7554539e-08 2.3645097e-07 ... 1.4111748e-23 5.1165764e-23\n",
      " 6.7833880e-36]\n",
      "[5.09993692e-09 1.22835195e-06 1.00272286e-04 ... 2.39975094e-07\n",
      " 8.45708144e-08 5.30180012e-13]\n",
      "[1.9283372e-23 1.4522778e-18 4.0912280e-13 ... 5.4348006e-35 3.9076861e-38\n",
      " 0.0000000e+00]\n",
      "[1.7963398e-11 5.5773595e-09 9.0514211e-04 ... 3.5081685e-28 6.4607571e-29\n",
      " 2.0022892e-29]\n",
      "[9.6420689e-18 3.3421306e-12 3.9079610e-14 ... 1.5005966e-21 1.8623197e-31\n",
      " 1.5764357e-31]\n",
      "[2.1360717e-10 1.3104197e-05 4.2980105e-06 ... 1.3967883e-17 8.0511371e-22\n",
      " 3.0042057e-19]\n",
      "[3.3443837e-10 1.5344612e-03 7.4145069e-06 ... 2.4873191e-27 0.0000000e+00\n",
      " 1.8829859e-19]\n",
      "[1.82806853e-10 5.77225210e-03 6.12046961e-06 ... 2.10863724e-18\n",
      " 1.27954325e-26 3.84713572e-12]\n",
      "[2.8933989e-09 1.8950421e-02 9.6532954e-03 ... 2.3747021e-25 2.2391472e-25\n",
      " 6.8397356e-24]\n",
      "im feeling chills me ever standing outta her time dull cant believe for you want me down tonight on the lights go down the fever screw of the might went shining liar and you deny it alive i been together the lights is going excite from brains like love rain fantasy smile from last through a key at steve had for the doin get courage fill rather buttered you see me down tonight tonight upon the rainbow under the wild side of town nights nights a shame has steady one lovers had giving need a doll jive singing hoping pushing to be to\n"
     ]
    }
   ],
   "source": [
    "# We generate lyrics again (2nd time, trying to get better results). This time we'll use a random function, so that we dont always get the predicted word with the \n",
    "# highest probability.\n",
    "seed_text = \"im feeling chills\"\n",
    "next_words = 100\n",
    "\n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted_probs = model.predict(token_list)[0]\n",
    "    print(predicted_probs)\n",
    "    predicted = np.random.choice([x for x in range(len(predicted_probs))],              # Before we used argmax, to get the higher probability obtained.\n",
    "                                    p=predicted_probs)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe68149fe8549fceca107e62d5023ede2ad6b4760e922d52c26df0ecc2fe7792"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
